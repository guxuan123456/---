# 贝叶斯分类器

前面学习了支持向量机，最大化间隔、对偶问题、SMO算法求解等等。引入核函数将低维度非线性变成高维线性分类，支持向量回归，软间隔支持向量机解决挤歪超平面的问题，这一章主要是介绍贝叶斯分类器，该分类器的主要优点是可以随意处理多分类任务。

## 贝叶斯决策论
贝叶斯决策论是概率框架下的实施决策的基本方法，对多分类任务说。在所有相关概率都已知的情况下，贝叶斯决策论考虑如何基于这些概率和误判损失来选择最优的判别标记。我们所做的决策应该是将样本分类正确，也就是说分类错误(风险)最小，下面我们来定义条件风险。

$\lambda _{i,j}$是将真是标记为$c_j$的样本标记为$c_i$所产生的损失，现在我们的样本是$x$，那么我们将样本标记为结果$c_j$的概率是$P(c_i|x)$，这样可以获得样本的条件风险：

$$ R(c_i|x) = \sum _{j=1}^N \lambda_{ij} P(c_j|x)$$

现在目标是寻找一个判定准则$h:X \rightarrow Y$，使上面的条件风险最小。

显然，若对于样本$x$,为最小化总体风险，只需在每个样本上选择那个使得条件风险最小的类标。

$$ h^*(x) = \underset{c \in y}{arg \, min} R(c|x)$$

若引入的损失函数为0-1损失，也就是当分类正确时风险为0，分类错误时风险为1就有前面的条件风险：

$$ R(c|x) = 1-P(c|x)$$

最小化分类错误的贝叶斯最优分类器为：

$$  h^*(x) = \underset{c \in y}{arg \, max} P(c|x)$$

上面式子直观理解就是让样本分类正确的概率最大，选择其后验概率$P（c | x）$最大所对应的类标。现在就是需要求$P(c|x)$，有两种求法：
- 判别式模型：直接对$P(c|x)$进行建模求解，感觉是利用训练集数据得到，前面的决策树、神经网络、支持向量机都是的。
- 生成式模型：先求联合分布$P(x,c)$，再求$P(c|x)$。但是目前我不知道二者各自的优点是啥。

贝叶斯分类器属于生成式模型，利用那个贝叶斯定理：

$$ P(c|x) = \frac{P(c) P(x|c)}{P(x)}$$

对于给定的样本$x$，$P(x)$与类标无关，$P(c)$称为类先验概率，是分类结果的比例。$p(x|c)$称为类条件概率。这时估计后验概率$P(c | x)$就变成为估计类先验概率和类条件概率的问题。对于先验概率和后验概率是什么东西：

- 先验概率：是指根据以往经验和分析得出的概率，一般通过频率统计可以得到。
- 后验概率：基于新的信息，就是一般是得到结果来求前面的概率(瞎说).一般要用到公式，后验概率就是条件概率。

实际上先验概率就是在没有任何结果出来的情况下估计的概率，而后验概率则是在有一定依据后的重新估计，直观意义上后验概率就是条件概率。

对于前面的公式：$p(c)$就是样本空间中各类样本所占的比例,可以使用各类出现的频率来代替.因此只剩下类条件概率$p(x | c )$，它表达的意思是在类别c中出现属性x的概率，它涉及属性的联合概率问题，若只有一个离散属性还好，当属性多时采用频率估计起来就十分困难，因此这里一般采用极大似然法进行估计。其实贝叶斯分类感觉主要是这个条件概率怎末求。

## 极大似然估计
极大似然估计（Maximum Likelihood Estimation，简称MLE），是一种根据数据采样来估计概率分布的经典方法。常用的策略是先假定总体具有某种确定的概率分布，再基于训练样本对概率分布的参数进行估计。运用到类条件概率p（x | c ）中，假设p（x | c ）服从一个参数为θ的分布，问题就变为根据已知的训练样本来估计θ。极大似然法的核心思想就是：估计出的参数使得已知样本出现的概率最大，即使得训练数据的似然最大。
<img src="picture\极大似然估计.png" width = 100% height = 50% />

所以，贝叶斯分类器的训练过程就是参数估计。总结最大似然法估计参数的过程，一般分为以下四个步骤：

1. 写出似然函数；
2. 对似然函数取对数，并整理；
3. 求导数，令偏导数为0，得到似然方程组；
4. 解似然方程组，得到所有参数即为所求。


例如：假设样本属性都是连续值，p（x | c ）服从一个多维高斯分布，则通过MLE计算出的参数刚好分别为：
<img src="picture\极大似然估计1.png" width = 80% height = 50% />

上述结果看起来十分合乎实际，但是采用最大似然法估计参数的效果很大程度上依赖于作出的假设是否合理，是否符合潜在的真实数据分布。这就需要大量的经验知识.

也就是说感觉极大似然估计就是自己根据经验知识假设出数据的分布形式，再根据分布形式和数据求出具体要求的东西。


## 朴素贝叶斯分类器

基于贝叶斯公式来估算后验概率$P(c|x)$难点在于类条件概率$P(x|c)$是所有属性上的联合概率，难以从有限数据获得。朴素贝叶斯分类器（naive Bayes classifier）采用了“属性条件独立性假设”，即样本数据的所有属性之间相互独立。这样类条件概率$p(x | c)$可以改写为所有属性的连乘积：
$$ P(c|x) = \frac{P(c)P(x|c)}{P(x)}  =\frac{P(c)}{P(x)} \prod_{i=1}^d P(x_i|c)$$

对所有类别来说$P(x)$相同，所以基于贝叶斯判定准则有：
$$ h_{nb}(x) = \underset{c \in y}{arg \, max} P(c) \prod _{i=1}^dP(x_i|c)$$

上式便是朴素贝叶斯分类器的表达式。上式需要求两个东西：
- 基于训练集D估计类先验概率
  $$ P(c) = \frac{|D_c|}{|D|}$$
- 每个属性估计条件概率
  $$P(x_i|c) = \frac{|D_{c,x_i}|}{|D_c|}$$

对于连续的属性值可以考虑概率密度函数，假定$p(x_i|c) ~ N(\mu_{c,i},\epsilon_{c,i}^2)$，则有：
$$ p(x_i|c) = \frac{1}{\sqrt{2 \pi} \epsilon_{c,i}} exp (- \frac{(x_i-\mu_{c,i})^2}{2\epsilon_{c,i}^2})$$

然后西瓜书上有一个计算的例子可以参考理解。上面式子有一个问题就是若某个属性值在训练集中和某个类别没有一起出现过，这样会抹掉其它的属性信息，因为该样本的类条件概率被计算为0，一乘就没了。因此在估计概率值时，常常用进行平滑（smoothing）处理，拉普拉斯修正（Laplacian correction）就是其中的一种经典方法，具体计算方法如下：
<img src="picture\拉普拉斯修正.png" width = 80% height = 50% />

当训练集越大时，拉普拉斯修正引入的影响越来越小。对于贝叶斯分类器，模型的训练就是参数估计，因此可以事先将所有的概率储存好，当有新样本需要判定时，直接查表计算即可。这里会有懒惰学习和增量学习的概念，后面再介绍。

## 半朴素贝叶斯分类器

前面的朴素贝叶斯分类器采用了属性条件独立性，但是现实任务中不是这样的，这种假设往往很难成立，现在考虑一些东西并不是独立的，由此产生"半朴素贝叶斯分类器"。

其中一个想对比较简单的是"独依赖估计"ODE，假设每个属性在类别之外最多仅依赖于一个其他属性，感觉这一部分的学习需要结合图论，在这里就没有深入学习，把西瓜书的这两页贴下来吧：
<img src="picture\半朴素贝叶斯分类器1.png" width = 80% height = 50% />
<img src="picture\半朴素贝叶斯分类器2.png" width = 80% height = 50% />


## 贝叶斯网络
贝叶斯网络又叫信念网，借助有向无环图刻画属性之间的依赖关系，使用条件概率表描述属性的联合概率分布，下面举一个例子简单的来说明一下。
<img src="picture\贝叶斯网络.png" width = 100% height = 50% />

具体来说，一个贝叶斯网络由结构和参数组成，结构就是大概那个图是什么形状，而参数则是根据该形状训练出来的参数，如下。
<img src="picture\贝叶斯网络1.png" width = 100% height = 50% />

### 结构
贝叶斯网络假设每个属性和他的非后裔属性独立，这里结构的训练分析也不是很懂，直接贴图：
<img src="picture\贝叶斯网络学习1.png" width = 80% height = 50% />
<img src="picture\贝叶斯网络学习2.png" width = 80% height = 50% />
<img src="picture\贝叶斯网络学习3.png" width = 80% height = 50% />
 
上面的东西不知道有何用处，后面如果用到贝叶斯网络时在进行深入学习。

### 学习
若网络结构已知，则属性间的依赖关系已知，只需通过对训练样本“计数”，估计出每个结点的条件概率表即可。但实际中并不知道网络结构。贝叶斯网学习的首要任务：根据训练数据集找出结构最恰当的贝叶斯网。常用解决方法：评分搜索。具体做法：先定义一个评分函数，以此来评估贝叶斯网和训练数据的契合程度，然后基于这个评分函数寻找结构最优的贝叶斯网。

常用评分函数通常基于信息论准则，将学习问题看作一个数据压缩任务，学习的目标是找一个能以最短编码长度描述训练数据的模型，此时编码的长度包括了描述模型自身所需的字节长度，以及使用该模型所需的字节长度。

最小描述长度准则：选择综合编码长度最短的贝叶斯网。
<img src="picture\贝叶斯网络学习4.png" width = 100% height = 50% />

感觉贝叶斯网络也不好理解(菜死了).直接贴图吧，有机会再来看：
<img src="picture\贝叶斯网络不会1.png" width = 100% height = 50% />
<img src="picture\贝叶斯网络不会2.png" width = 100% height = 50% />

下面整一个课件上的例子：
<img src="picture\贝叶斯网络不会3.png" width = 100% height = 50% />
<img src="picture\贝叶斯网络不会4.png" width = 100% height = 50% />
<img src="picture\贝叶斯网络不会5.png" width = 100% height = 50% />

### 推断
<img src="picture\贝叶斯网络应用1.png" width = 100% height = 50% />

下面我们分别看看贝叶斯网络如何应用在诊断和预测上


#### 预测
<img src="picture\贝叶斯网络预测1.png" width = 100% height = 50% />

预测算法的一个例子：
<img src="picture\贝叶斯网络预测2.png" width = 100% height = 50% />
<img src="picture\贝叶斯网络预测3.png" width = 100% height = 50% />


#### 诊断
<img src="picture\贝叶斯网络诊断1.png" width = 100% height = 50% />  
诊断算法的一个例子：
<img src="picture\贝叶斯网络诊断2.png" width = 100% height = 50% />
<img src="picture\贝叶斯网络诊断5.png" width = 100% height = 50% />


## EM算法

这个算法在数学之美上接触了，感觉这个算法不进行实际操作时很难理解透彻。

EM（Expectation-Maximization）算法是一种常用的估计参数隐变量的利器，也称为“期望最大算法”，是数据挖掘的十大经典算法之一。EM算法主要应用于训练集样本不完整即存在隐变量时的情形（例如某个属性值未知），通过其独特的“两步走”策略能较好地估计出隐变量的值。

EM是一种迭代式的方法，它的基本思想就是：若样本服从的分布参数θ已知，则可以根据已观测到的训练样本推断出隐变量Z的期望值（E步），若Z的值已知则运用最大似然法估计出新的θ值（M步）。重复这个过程直到Z和θ值不再发生变化。(感觉这个东西跟自己最近看的一个论文相似)。


简单来讲：假设我们想估计A和B这两个参数，在开始状态下二者都是未知的，但如果知道了A的信息就可以得到B的信息，反过来知道了B也就得到了A。可以考虑首先赋予A某种初值，以此得到B的估计值，然后从B的当前值出发，重新估计A的取值，这个过程一直持续到收敛为止。

现在再来回想聚类的代表算法K-Means：【首先随机选择类中心=>将样本点划分到类簇中=>重新计算类中心=>不断迭代直至收敛】，不难发现这个过程和EM迭代的方法极其相似，事实上，若将样本的类别看做为“隐变量”（latent variable）Z，类中心看作样本的分布参数θ，K-Means就是通过EM算法来进行迭代的，与我们这里不同的是，K-Means的目标是最小化样本点到其对应类中心的距离和，上述为极大化似然函数。


下面是关于EM算法的推导和实现(直接copy过来的，感觉蛮复杂，但是其原理在前面已经说清楚了，就是反复优化两个参数)，下面直接复制过来啦。

### 推导和求解
在上篇极大似然法中，当样本属性值都已知时，我们很容易通过极大化对数似然，接着对每个参数求偏导计算出参数的值。但当存在隐变量时，就无法直接求解，此时我们通常最大化已观察数据的对数“边际似然”（marginal likelihood）。
<img src="picture\EM1.png" width = 100% height = 50% />

这时候，通过边缘似然将隐变量Z引入进来，对于参数估计，现在与最大似然不同的只是似然函数式中多了一个未知的变量Z，也就是说我们的目标是找到适合的θ和Z让L(θ)最大，这样我们也可以分别对未知的θ和Z求偏导，再令其等于0。

然而观察上式可以发现，和的对数（ln(x1+x2+x3)）求导十分复杂，那能否通过变换上式得到一种求导简单的新表达式呢？这时候 Jensen不等式就派上用场了，先回顾一下高等数学凸函数的内容：

**Jensen's inequality**：过一个凸函数上任意两点所作割线一定在这两点间的函数图象的上方。理解起来也十分简单，对于凸函数f(x)''>0，即曲线的变化率是越来越大单调递增的，所以函数越到后面增长越厉害，这样在一个区间下，函数的均值就会大一些了。
<img src="picture\EM2.png" width = 100% height = 50% />

因为ln(*)函数为凹函数，故可以将上式“和的对数”变为“对数的和”，这样就很容易求导了。
<img src="picture\EM3.png" width = 100% height = 50% />

接着求解Qi和θ：首先固定θ（初始值），通过求解Qi使得J（θ，Q）在θ处与L（θ）相等，即求出L（θ）的下界；然后再固定Qi，调整θ，最大化下界J（θ，Q）。不断重复两个步骤直到稳定。通过jensen不等式的性质，Qi的计算公式实际上就是后验概率：
<img src="picture\EM6.png" width = 100% height = 50% />

通过数学公式的推导，简单来理解这一过程：固定θ计算Q的过程就是在建立L（θ）的下界，即通过jenson不等式得到的下界（E步）；固定Q计算θ则是使得下界极大化（M步），从而不断推高边缘似然L（θ）。从而循序渐进地计算出L（θ）取得极大值时隐变量Z的估计值。

EM算法也可以看作一种“坐标下降法”，首先固定一个值，对另外一个值求极值，不断重复直到收敛。这时候也许大家就有疑问，问什么不直接这两个家伙求偏导用梯度下降呢？这时候就是坐标下降的优势，有些特殊的函数，例如曲线函数z=y^2+x^2+x^2y+xy+...，无法直接求导，这时如果先固定其中的一个变量，再对另一个变量求极值，则变得可行。
<img src="picture\EM4.png" width = 100% height = 50% />

- EM算法流程
<img src="picture\EM5.png" width = 100% height = 50% />










