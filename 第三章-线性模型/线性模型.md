# 第三章、线性模型
前面两章主要讲述了一些预备知识，没有涉及相应算法和处理方法，讲述了机器学习的定义、术语、学习器性能的估计度量和比较，基本上可以说前面东西是对机器学习的了解。之后我们就是了解一些更有用的东西了，可以了解具体算法然后进行试验。

## 基本形式
给定由$d$个属性描述的示例$x = (x_1;x_2;...;x_d)$，其中$x_i$是$x$在第$i$个属性上的取值，线性模型就是试图学的一个通过属性的线性组合来进行预测的函数，如下：
$$ f(x) = w_1x_1+w_2x_2+...+w_dx_d+b$$

写成向量形式$$ f(x) = w^T+b$$

其中$w=(w_1;w_2;...;w_d)$.$w和d$学得之后上述模型就得以确定。

上述线性模型看上去也比较简单，实际上高中就接触过，最小二乘法便是很简单的线性回归，属性值为1.且式子中的$w$代表各属性在模型中所占的比率。本章我们就搞几种经典的线性模型，从回归任务开始，接着讨论分类和多分类问题。。

##  线性回归
线性回归问题就是试图学到一个线性模型尽可能准确地预测新样本的输出值，就是求上面的那个表达式。

输入的属性值并不能总是直接被我们的学习模型所用，需要进行相应的处理，对于连续值的属性，一般都可以被学习器所用，有时会根据具体的情形作相应的预处理，例如：归一化等；对于离散值的属性，可作下面的处理：

- 若属性值之间存在“序关系”，则可以将其转化为连续值，例如：身高属性分为“高”“中等”“矮”，可转化为数值：{1， 0.5， 0}。

- 若属性值之间不存在“序关系”，则通常将其转化为向量的形式，例如：性别属性分为“男”“女”，可转化为二维向量：{（1，0），（0，1）}。

线性模型主要是求w和b，这两个东西主要是受属性值个数的影响，我们先来讨论比较简单的情况，也就是输入只有一个属性值。

- 输入只有一个属性值
    一般机器学习求解过程都是确定目标函数和约束条件，然后求解，在只有一个属性值时，线性回归要学的就是：
    $$ f(x_i) = w_ix_i+b，使得f(x_i) \approx y_i$$

    前面我们知道衡量好坏的一个标准是均方误差最小，我们的目标就是让均方误差最小来求相应的w和b，下面是具体计算步骤：
    <img src="picture\线性回归1.png" width = 100% height = 70% />
    
    上面一堆求导取零操作便得到了相应的值,其中$\bar x = \frac{1}{m}\sum_{i=1}^m x_i$，这边是我们熟悉的最小二乘法
- 输入多个属性值
    当输入属性有多个的时候，例如对于一个样本有d个属性${(x_1,x_2...x_d),y}$，则$y=wx+b$需要变成：
    $$ f(x_i) = w^Tx_i+b_i 使得f(x_i) \approx y_i$$
    上面这个东西是多元线性回归，一般都是多属性。对于多元，科学家们一般喜欢使用矩阵来表达，所以线性代数要好好学，相应的公式推导线性代数没学过的可能看着有些苦难。

    我们也可以利用最小二乘法对$w,b$进行估计，换成矩阵，我们用$\hat{w} = (w;b)$把数据集$D$表示为一个$m \times (d+1)$的矩阵$X$，如下图所示：
    <img src="picture\线性回归2.png" width = 60% height = 70% />

    相应的乘积就是下面这样，这样也将$y$写成向量形式$y = (y_1;y_2;...;y_m)$:
    <img src="picture\线性回归3.png" width = 80% height = 70% />

    得到的目标函数如下：
    <img src="picture\线性回归4.png" width = 50% height = 70% />

    同样方式我们可以将均方误差求导得到相应的参数值，这里矩阵求导我还不太熟悉，另外这里需要注意的是当一个矩阵的行列式不等于0时，我们才可能对其求逆，因此对于下式，我们需要考虑矩阵（X的转置*X）的行列式是否为0，若不为0，则可以求出其解，若为0，则需要使用其它的方法进行计算，书中提到了引入正则化，此处不进行深入。
     <img src="picture\线性回归5.png" width = 60% height = 70% />
    
    这样我们就得到线性回归模型了，这次是很多属性的了。

有时像上面这种原始的线性回归可能并不能满足需求，例如：y值并不是线性变化，而是在指数尺度上变化。这时我们可以采用线性模型来逼近y的衍生物，例如$lny$，这时衍生的线性模型如下所示，实际上就是相当于将指数曲线投影在一条直线上，如下图所示：

 <img src="picture\线性回归6.png" width = 60% height = 40% />

更一般地，考虑所有y的衍生物的情形，就得到了“广义的线性模型”（generalized linear model），其中，$g(*)$称为联系函数(link function)。
$$ y = g^{-1}(w^Tx+b)$$

显然，对数线性回归是广义模型的一个特例。
## 对数几率回归

回归就是通过输入的属性值得到一个预测值，利用上述广义线性模型的特征，可以整一个联系函数将预测的连续值换成离散值，这样就可以完成分类任务了，好的，现在我们来整一个二分类任务。

线性回归产生的预测值是$z = w^Tx+b$，我们考虑将实数z转换成0/1，理想的的是单位阶跃函数。但是单位阶跃函数不连续，就不能用那个广义线性模型的函数了，现在我们找一个对数几率函数替代单位阶跃函数，其表达式和相应图像如下：

$$ y = \frac{1}{1+e^{-z}}$$
 <img src="picture\对数几率函数1.png" width = 80% height = 70% />

这样原来的线性回归函数就变成了下面这样：
<img src="picture\对数几率函数2.png" width = 80% height = 70% />

若将y看做样本为正例的概率，（1-y）看做样本为反例的概率，则上式实际上使用线性回归模型的预测结果器逼近真实标记的对数几率。因此这个模型称为“对数几率回归”（logistic regression），也有一些书籍称之为“逻辑回归”。下面使用最大似然估计的方法来计算出$w和b$两个参数的取值，下面只列出求解的思路，不列出具体的计算过程，因为计算过程我不会啊。

我们将$y$视为类后验概率估计$p(y=1|x)$,则有：
 <img src="picture\对数几率函数3.png" width = 80% height = 70% />
经过极大似然法之后：
 <img src="picture\对数几率函数4.png" width = 80% height = 70% />

上面整出来了目标函数，接下来就是求值了，都是矩阵所以需要先用其他矩阵替换一下，然后化简目标函数，利用梯度下降法或者牛顿法算出最优解，当然这两个方法我还不会，可能后续会学，现在主要是把书上的这一部分贴上去吧。

 <img src="picture\对数几率函数5.png" width = 80% height = 70% />

 <img src="picture\对数几率函数6.png" width = 100% height = 30% />

## 线性判别分析

线性判别分析（Linear Discriminant Analysis，简称LDA）,其基本思想是：将训练样本投影到一条直线上，使得同类的样例尽可能近，不同类的样例尽可能远，感觉有点像聚类。如图所示：
<img src="picture\线性判别分析.png" width = 80% height = 70% />

其中“+”、“-”代表正例和反例，椭圆表示数据簇的外轮廓，虚线表示投影，红色实心圆和实心三角形表示两类样本投影后的中心点。
<img src="picture\线性判别分析1.png" width = 100% height = 70% />

想让同类样本点的投影点尽可能接近，不同类样本点投影之间尽可能远，即：让各类的协方差之和$w^T\sum_0 w+ w^T\sum_1 w$尽可能小，不用类之间中心的距离尽可能大$||w^T\mu_0-w^T\mu_1||_2^2$。同时考虑两方面，得到优化目标：
$$ J = \frac{||w^T\mu_0-w^T\mu_1||_2^2}{w^T\sum_0 w+ w^T\sum_1 w}$$

为了化简我们的上述目标函数，LDA定义了两个散度矩阵。

+ 类内散度矩阵（within-class scatter matrix）
<img src="picture\线性判别分析2.png" width = 80% height = 70% />

+ 类间散度矩阵(between-class scaltter matrix)
<img src="picture\线性判别分析3.png" width = 80% height = 70% />

因此得到了LDA的最大化目标：“广义瑞利商”（generalized Rayleigh quotient）。
<img src="picture\线性判别分析4.png" width = 80% height = 70% />

从而分类问题转化为最优化求解w的问题，当求解出w后，对新的样本进行分类时，只需将该样本点投影到这条直线上，根据与各个类别的中心值进行比较，从而判定出新样本与哪个类别距离最近。求解w的方法如下所示，使用的方法为λ乘子。
<img src="picture\线性判别分析5.png" width = 80% height = 70% />

上面是二分类问题，我们研究问题一般都是从二分类到多分类，下面将LDA推广到多分类。

存在$N$个类，第$i$类示例数为$m_i$，定义全局散度矩阵：
$$ S_t = S_b + S_w = \sum_{i=1}^m(x_i-\mu)(x_i-\mu)^T$$

其中$\mu$是所有实例中的均值向量，将类内散度矩阵$S_w$定义为每个类别的散度矩阵之和
$$ S_w = \sum_{i =1}^NS_{w_i}$$

其中$S_{w_i} = \sum _{x \in X_i} (x-\mu_i)(x-\mu_i)^T$。可得：

$$S_b = S_t-S_w = \sum_{i=1}^N(\mu_i-\mu)(\mu_i-\mu)^T$$

最优化目标：
$$\underset{W}{\operatorname{max}} \frac{tr(W^TS_bW)}{tr(W^TS_wW)}  $$

W的闭式解则是$S_w^{-1}S_b$的$N-1$个最大广义特征值所对应的特征向量组成的矩阵(很明显这句话我听不懂，因为我数学不好啊！！！！)

若将w看做一个投影矩阵，类似PCA的思想，则LDA可将样本投影到N-1维空间（N为类簇数），投影的过程使用了类别信息（标记信息），因此LDA也常被视为一种经典的监督降维技术。             
## 多分类学习

现实中我们经常遇到不只两个类别的分类问题，即多分类问题，在这种情形下，我们常常运用“拆分”的策略，通过多个二分类学习器来解决多分类问题，即将多分类问题拆解为多个二分类问题，训练出多个二分类学习器，最后将多个分类结果进行集成得出结论。最为经典的拆分策略有三种：“一对一”（OvO）、“一对其余”（OvR）和“多对多”（MvM），核心思想与示意图如下所示。

+ OvO：给定数据集D，假定其中有N个真实类别，将这N个类别进行两两配对（一个正类/一个反类），从而产生N（N-1）/2个二分类学习器，在测试阶段，将新样本放入所有的二分类学习器中测试，得出N（N-1）个结果，最终通过投票产生最终的分类结果。

+ OvR：给定数据集D，假定其中有N个真实类别，每次取出一个类作为正类，剩余的所有类别作为一个新的反类，从而产生N个二分类学习器，在测试阶段，得出N个结果，若仅有一个学习器预测为正类，则对应的类标作为最终分类结果。

+ MvM：给定数据集D，假定其中有N个真实类别，每次取若干个类作为正类，若干个类作为反类（通过ECOC码给出，编码），若进行了M次划分，则生成了M个二分类学习器，在测试阶段（解码），得出M个结果组成一个新的码，最终通过计算海明/欧式距离选择距离最小的类别作为最终分类结果。
<img src="picture\多分类学习.png" width = 100% height = 70% />
OVO策略和OvR策略之间优缺点比较如下：
<img src="picture\多分类学习1.png" width = 90% height = 70% />

下面介绍一下MvM最常用的一个技术：“纠错输出码”(ECOC)，将编码方式引入类别拆分，在解码过程还具有容错性，其工作过程主要分两步：
<img src="picture\多分类学习2.png" width = 90% height = 70% />

常见的编码矩阵主要有二元码三源码。
<img src="picture\多分类学习3.png" width = 100% height = 70% />

该方法具有一定的纠错能力：
<img src="picture\多分类学习4.png" width = 100% height = 70% />

## 类别不平衡问题

类别不平衡（class-imbanlance）就是指分类问题中不同类别的训练样本相差悬殊的情况，例如正例有900个，而反例只有100个，这个时候我们就需要进行相应的处理来平衡这个问题。常见的做法有三种：

1.  在训练样本较多的类别中进行“欠采样”（undersampling）,但是这样可能会造成重要信息丢失，但是计算时间短。比如从正例中采出100个，常见的算法有：EasyEnsemble，将反例划分为若干个集合共不同学习器使用，对每个学习器都采用的欠采样但是总体没有丢失任何信息。
2.  在训练样本较少的类别中进行“过采样”（oversampling）,例如通过对反例中的数据进行插值，来产生额外的反例，但是不能盲目加，不然会产生过拟合，常见的算法有SMOTE。
3.  直接基于原数据集进行学习，对预测值进行“再缩放”处理。其中再缩放也是代价敏感学习的基础。也叫阈值移动。
   
下面详细说说相应的阈值移动的东西：

<img src="picture\类别不平衡问题1.png" width = 100% height = 50% />
<img src="picture\类别不平衡问题2.png" width = 100% height = 10% />

好的，到目前为止，线性规划的我们整完了，因为公式好多啊！所以我大部分贴的图，当然也有白嫖别人的东西。接下来我们可能就是手动编程实现一下相关算法了，当然牛逼的同学可以写西瓜书课后习题，反正我是不会写。下一阶段，决策树！

## 补充：梯度下降法

梯度下降法又叫最快速下降法

基本思想类似于我么爬山，如果想最快到达山顶或者山底，最快的方式是沿着最陡峭的方向。这样从任意点出发，需要最快搜索到函数最大值，也应该是函数变化最快的方向搜索。

函数变化最快的方向就是函数的梯度，如果要找函数最小值，，从负梯度方向找，该方法叫梯度下降法，一元函数的梯度为该函数的倒数。

梯度下降法的一般步骤：
<img src="picture\梯度下降法1.png" width = 90% height = 10% />

在线性回归时，假设$d=1,b=0$代价函数是关于$w$的二次函数：
$$ J(w,0) = \frac{1}{2}(h(x)- y)^2=\frac{1}{2}(wx-y)^2$$

​根据问题的不同应设置不同的学习速度，如果代价函数平滑，梯度很小，可以设置较大的    $\alpha$以尽快收敛，如果函数很陡峭，可以设置稍小的$\alpha$以至于不跳出最优解。                                                  
<img src="picture\梯度下降法2.png" width = 100% height = 10% />

下面是二元函数和多元函数利用梯度下降法的情况：
<img src="picture\梯度下降法3.png" width = 100% height = 10% />

- 局部最优和全局最优
    顾名思义，局部最优就是局部最小值，全局就是全局最小值。
    1. 代价函数可以有多个局部最优解，通常局部最优解不对应全局最优解。
    2. 从不同的起始点出发梯度下降会收敛到不同的局部最优位置。
    3. 可以多次随机初始化初始点，取多次迭代结果最小值作为最后输出解。


还有一个更牛逼的地方我慢慢学会使用VS code进行python开发了，由于机器学习主要还是采用python开发，所以最近一段时间会一直使用python，通过vs code既能写文档也能使用python，好像其它语言也都可以。今后如果可以的话我要将VS code开发python程序写成一篇文章。




