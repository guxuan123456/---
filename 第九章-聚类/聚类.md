# 聚类
#### 上章回顾小结：
上篇主要介绍了一种机器学习的通用框架--集成学习方法，首先从准确性和差异性两个重要概念引出集成学习“**好而不同**”的四字真言，接着介绍了现阶段主流的三种集成学习方法：AdaBoost、Bagging及Random Forest，AdaBoost采用最小化指数损失函数迭代式更新样本分布权重和计算基学习器权重，Bagging通过自助采样引入样本扰动增加了基学习器之间的差异性，随机森林则进一步引入了属性扰动，最后简单概述了集成模型中的三类结合策略：平均法、投票法及学习法，其中Stacking是学习法的典型代表。本篇将讨论无监督学习中应用最为广泛的学习算法--聚类。原来聚类我在数学之美中接触过一点，数学之美讲的通俗易懂，但是西瓜书就更接近原理的，并且讲的也很详细，因此理解起来更难。

## 聚类任务

先说聚类是一种**无监督学习**方法。**无监督学习的目标是通过对无标记训练样本的学习，发掘和揭示数据集本身潜在的结构与规律**，即不依赖于训练数据集的类标记信息。聚类则是试图将数据集的样本划分为若干个互不相交的类簇，从而每个簇对应一个潜在的类别。

上面说的概念比较官方，简单来说聚类就是将一堆没有标记的样本进行分类的，就是把相似的样本放在一类，形成一个**类簇**，这样的话一堆数据我们就能分成许多个类。

基于不同的学习策略我们可以设计出多种类型的聚类算法，后面会有许多不同类型的代表性算法。那么聚类的一个重大问题是如何来**度量相似性**（similarity measure）呢？这便是**距离度量**，在生活中我们说差别小则相似，对应到多维样本，每个样本可以对应于高维空间中的一个数据点，若它们的距离相近，我们便可以称它们相似。那接着如何来评价聚类结果的好坏呢？这便是**性能度量**，性能度量为评价聚类结果的好坏提供了一系列有效性指标。

下面是数学形式的聚类的概念：
<img src="picture\聚类任务1.png" width =90% height = 50%>


## 性能度量
也称为聚类的有效性指标，用来评估聚类结果的好坏。如何度量，不同类的样本尽可能不同，相同类的尽可能相同。那么簇内相似度和簇间相似度就需要被整出来。

由于聚类算法不依赖于样本的真实类标，就不能像监督学习的分类那般，通过计算分对分错（即精确度或错误率）来评价学习器的好坏或作为学习过程中的优化目标。一般聚类有两类性能度量指标：**外部指标**(将聚类结果与某个参考模型进行比较)和**内部指标**(直接考察聚类结果而不利用任何参考模型)。

### 外部指标
外部指标我们需要整一个参考模型作为比较，以该参考模型的输出作为标准来评价聚类好坏。假设聚类给出的结果为λ，参考模型给出的结果是λ*，则我们将样本进行两两配对，定义：
<img src="picture\外部指标1.png" width =70% height = 50%>

由于每个样本对$(x_i,x_j)(i<j)$仅能出现在一个集合中，因此有：
$$ a+b+c+d = \frac{m(m-1)}{2}$$

然后我们定义下面外部指标：
<img src="picture\外部指标2.png" width =70% height = 50%>

### 内部指标
现在来看看内部指标。直接对聚类的结果进行评估，聚类的目的是想将那些相似的样本尽可能聚在一起，不相似的样本尽可能分开。
<img src="picture\内部指标1.png" width =70% height = 50%>

其中$dist(.,.)$为两个样本之间的距离，基于上面的距离定义，可以得出下面的常用的内部评价指标：
<img src="picture\内部指标2.png" width =70% height = 50%>


## 距离计算
距离度量$dist(.,.)$通常具有如下性质：
<img src="picture\距离度量1.png" width =70% height = 50%>

给定样本$x_i = (x_{i1},x_{i2};...;x_{in})$,与$x_j = (x_{j1};x_{j2},...,x_{jn})$.最常用的距离度量方法是“闵可夫斯基距离”（Minkowski distance)：
<img src="picture\闵可夫斯基距离1.png" width =70% height = 50%>

上式当$p=2$时就是欧氏距离（Euclidean distance）：
<img src="picture\欧氏距离.png" width =70% height = 50%>

当$p=1$时就是曼哈顿距离（Manhattan distance）：
<img src="picture\曼哈顿距离.png" width =70% height = 50%>

属性值分为连续属性和离散属性，连续属性一般可以直接计算距离，当离散属不能直接计算距离时，需进行处理。
> 若属性值之间**存在序关系**，则可以将其转化为连续值，例如：身高属性“高”“中等”“矮”，可转化为{1, 0.5, 0}。
> 若属性值之间**不存在序关系**，则通常将其转化为向量的形式，例如：性别属性“男”“女”，可转化为{（1,0），（0,1）}。

另外，属性分为有序属性和无序属性。
- 有序属性：直接在属性值上计算距离，采用闵可夫斯基距离。
- 无序属性：不能直接计算距离，采用VDM距离

下面是属性u上两个离散值a与b之间的VDM距离：
<img src="picture\VDM距离.png" width =70% height = 50%>

其中$m_{u,a}$表示在属性$u$上取值$a$的样本数，$m_{u,a,i}$表示第i个样本簇中在属性u上取值为a的样本数，k为样本簇数。

于是，在计算两个样本之间的距离时，我们可以将闵可夫斯基距离和VDM混合在一起进行计算：
<img src="picture\混合距离.png" width =70% height = 50%>

当样本空间不同属性重要性不同时可使用加权距离：
$$ dist_{wmk} (x_i,x_j) = (w_i \cdot |x_{i1}-x_{j1}|^p+...+w_n \cdot |x_{in} -x_{jn}|^P)^{\frac{1}{P}}$$

若我们定义的距离计算方法是用来度量相似性，例如下面将要讨论的聚类问题，即距离越小，相似性越大，反之距离越大，相似性越小。这时距离的度量方法并不一定需要满足前面所说的四个基本性质，这样的方法称为：**非度量距离（non-metric distance）**。

## 原型聚类
原型聚类就是基于原型的聚类，就是通过参考一个模板向量或者模板分布的方式来完成聚类过程。常见的原型聚类算法包括：
1. K 均值算法：K means
2. 学习向量量化：Learning Vector Quantization (LVQ)
3. 高斯混合聚类：Mixture-of-Gaussian

下面分别介绍三种方法：
### K均值算法
主要思想：把$n$个点划分到$k$个聚类中，使得每个点都属于离他最近的聚类中心。

首先随机指定类中心，根据样本与类中心的远近划分类簇，接着重新计算类中心，迭代直至收敛。

给定样本集$D = {x_1,x_2,...,x_m}$,k均值算法就是针对聚类所得簇划分$C={C_1,C_2,...,C_k}$最小化平方误差：

$$ E = \sum _{i=1}^k \sum _{x \in C_i} ||x-\mu _i||^2_2$$

其中$\mu_i$是簇$C_i$的均值向量。K-Means的算法流程如下所示：
<img src="picture\kmean算法.png" width =80% height = 50%>

其实k均值算法相对来说很好理解，某种程度上来说它是一种EM算法，这在数学之美上作者有所介绍，关于k均值算法，课本上有一个例子可以加深理解。这里不再赘述，下面贴了一下k均值算法的优缺点。
<img src="picture\kmean算法优缺点.png" width =70% height = 50%>

### 学习向量量化（LVQ）
LVQ 假设数据样本带有类别标记，学习过程利用
样本的这些监督信息来辅助聚类。**LVQ使用样本真实类标记辅助聚类**，首先LVQ根据样本的类标记，从各类中分别随机选出一个样本作为该类簇的原型，从而组成了一个**原型特征向量组**，接着从样本集中随机挑选一个样本，计算其与原型向量组中每个向量的距离，并选取距离最小的原型向量所在的类簇作为它的划分结果，再与真实类标比较。

- 若划分结果正确，则对应原型向量向这个样本靠近一些
- 若划分结果不正确，则对应原型向量向这个样本远离一些

LVQ算法的流程如下所示：
<img src="picture\LVQ算法.png" width =80% height = 50%>

这个算法也是蛮好理解，西瓜书上也有一个该算法的例子，感兴趣的可以加深理解，这里不再赘述。看下一个算法-高斯混合聚类。

### 高斯混合聚类

可以看出K-Means与LVQ都试图以类中心作为原型指导聚类，高斯混合聚类则采用高斯分布来描述原型。现假设**每个类簇中的样本都服从一个多维高斯分布，那么空间中的样本可以看作由k个多维高斯分布混合而成**。高斯混合 (Mixture-of-Gaussian)聚类采用概率模型表达聚类原型。

下面是多维高斯分布的概率密度函数：
<img src="picture\多维高斯分布.png" width =70% height = 50%>

其中$\mu$表示均值向量，$∑$表示协方差矩阵，可以看出一个多维高斯分布完全由这两个参数所确定，现将概率密度函数记为$p(x|(\mu,\sum))$。接着定义高斯混合分布为：
<img src="picture\高斯混合分布.png" width =70% height = 50%>
该分布由k 个混合成分组成，每个成分对应于一个高斯分布.$\mu_i,\sum_i$是第i个高斯混合成分的参数，$\alpha_i$维混合系数，$\sum_{i=1}^k \alpha_i =1$.这样空间中样本的采集过程则可以抽象为：
- 先选择一个类簇（高斯分布）
- 再根据对应高斯分布的密度函数进行采样

上面的算法实际就是一个EM算法，可以发现，我们在这个模型中就是要得到高斯分布的表达式，然后得到样本$x_j$由第i个高斯成分生成的后验概率大小，然后我们选取最大的那个高斯成分作为我们的标记簇，之后我们需要更新高斯混合分布的表达式，也就是我们需要更新参数$\mu_i,\sum_i,\alpha_i$.之后同样进行上面操作，直至最后达到迭代次数或者参数更新基本不变。

具体公式推导过程可以参考西瓜书上的内容，这里展示一下三个参数的更新式子。
<img src="picture\混合高斯分布1.png" width =70% height = 50%>

其中$\gamma _{ji} = p_M(z_j=i|x_j)$，满足下面的式子：
<img src="picture\高斯混合分布2.png" width =70% height = 50%>

当然在每一轮迭代中，每个样本$x_j$的簇标记$\lambda_j$如下确定：
$$ \lambda _j = \underset{i \in {\{1,2,...,k\}}}{arg \, max} \gamma_{ji}$$
高斯混合聚类的算法流程如下图所示：
<img src="picture\高斯混合分布3.png" width =70% height = 50%>

西瓜书上也有一个例子可以加强理解，可以参考。
## 密度聚类
密度聚类则是基于密度的聚类，它从样本分布的角度来考察样本之间的可连接性，并基于可连接性（密度可达）不断拓展疆域（类簇）。其中最著名的便是**DBSCAN**算法，密度聚类具有如下特点：

 能克服基于距离的算法只能发现“类圆形”的聚类的缺点，可发现任意形状的聚类，且对噪声数据不敏感。但计算密度单元的计算复杂度大，需要建立空间索引来降低计算量，且对数据维数的伸缩性较差。

首先定义以下概念：
<img src="picture\密度聚类1.png" width =70% height = 50%>
<img src="picture\密度聚类2.png" width =70% height = 50%>

简单来理解DBSCAN便是：**找出一个核心对象所有密度可达的样本集合形成簇**。首先从数据集中任选一个核心对象A，找出所有A密度可达的样本集合，将这些样本形成一个密度相连的类簇，直到所有的核心对象都遍历完。DBSCAN算法的流程如下图所示：
<img src="picture\密度聚类3.png" width =70% height = 50%>

西瓜书也有个例子可以加强理解，但我感觉密度聚类理解起来应该不难。
## 层次聚类

层次聚类是一种基于树形结构的聚类方法，该方法也好理解，试图在不同层次对数据集进行划分，从而形成树形的聚类结构。数据集的划分可采用“自底向上”的聚合策略，也可采用“自顶向下”的分拆策略.

- 自顶向下分裂
  首先将所有对象置于一个簇中，然后逐渐细分为越来越小的簇，直至达到了某个终结条件。
- 自底向上凝聚
  首先将每个对象作为一个簇，然后合并这些原子簇为越来越大的簇，直至某个终结条件被满足。

其自底向上的一个算法是**AGNES算法**。步骤如下：
> 1.初始化-->把每个样本归为一类，计算每两个类之间的距离，也就是样本与样本之间的相似度；
> 2.寻找各个类之间最近的两个类，把他们归为一类（这样类的总数就少了一个）；
> 3.重新计算新生成的这个**类与各个旧类之间的相似度**；
> 4.重复2和3直到所有样本点都归为一类，结束。

可以看出其中最关键的一步就是**计算两个类簇的距离**，这里有多种度量方法：
-  单链接（single-linkage）:取类间最小距离。
  <img src="picture\单链接.png" width =70% height = 50%>
- 全链接（complete-linkage）:取类间最大距离
  <img src="picture\全链接.png" width =70% height = 50%>
- 均链接（average-linkage）:取类间两两的平均距离
  <img src="picture\均链接.png" width =70% height = 50%>

层次聚类法的算法流程如下所示：
<img src="picture\层次聚类.png" width =70% height = 50%>

该部分西瓜书上同样有个例子可以加强理解。