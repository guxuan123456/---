# 2  模型的评估与选择

## 2.1 经验误差与过拟合

误差：学习器的实际预测输出与样本的真实值之间的差异。定义：	

 - 在训练集上的误差称为训练误差（training error）或经验误差（empirical error）。
 - 在测试集上的误差称为测试误差（test error）。
 - 学习器在所有新样本上的误差称为泛化误差（generalization error）。
<img src="picture\误差1.png" width = 80% height = 70% />

显然，我们希望得到的是在新样本上表现得很好的学习器，即泛化误差小的学习器。因此，我们应该让学习器尽可能地从训练集中学出普适性的“一般特征”，这样在遇到新样本时才能做出正确的判别。然而，当学习器把训练集学得“太好”的时候，即把一些训练样本的自身特点当做了普遍特征；同时也有学习能力不足的情况，即训练集的基本特征都没有学习出来。因此有过拟合和欠拟合：

 - 学习能力过强，以至于把训练样本所包含的不太一般的特性都学到了，称为：过拟合（overfitting）。
 - 学习能太差，训练样本的一般性质尚未学好，称为：欠拟合（underfitting）。

可以得知：在过拟合问题中，训练误差十分小，但测试误差大；在欠拟合问题中，训练误差和测试误差都比较大。目前，欠拟合问题比较容易克服，例如增加迭代次数等，但过拟合问题还没有十分好的解决方案，过拟合是机器学习面临的关键障碍。
<img src="picture\误差2.png" width = 80% height = 70% />

## 2.2 评估方法
我们希望得到的是泛化误差小的学习器，理想的解决方案是对模型的泛化误差进行评估，然后选择泛化误差最小的那个学习器。但是，泛化误差指的是模型在所有新样本上的适用能力，我们无法直接获得泛化误差。

因此，通常我们采用一个“测试集”来测试学习器对新样本的判别能力，然后以“测试集”上的“测试误差”作为“泛化误差”的近似。显然：我们选取的测试集应尽可能与训练集互斥，整个例子来解释一下：

假设老师出了10 道习题供同学们练习，考试时老师又用同样的这10道题作为试题，可能有的童鞋只会做这10 道题却能得高分，很明显：这个考试成绩并不能有效地反映出真实水平。回到我们的问题上来，我们希望得到泛化性能好的模型，好比希望同学们课程学得好并获得了对所学知识"举一反三"的能力；训练样本相当于给同学们练习的习题，测试过程则相当于考试。显然，若测试样本被用作训练了，则得到的将是过于"乐观"的估计结果。

### 训练集与测试集的划分方法

整一个“测试集”的“测试误差”来作为“泛化误差”的近似，因此我们需要对初始数据集进行有效划分，划分出互斥的“训练集”和“测试集”。常见的划分方法如下：

- 留出法

将数据集D划分为两个互斥的集合，一个作为训练集$S$，一个作为测试集$T$，满足$D=S \cup T且S∩T=\emptyset$，常见的划分为：大约2/3-4/5的样本用作训练，剩下的用作测试。需要注意的是：训练/测试集的划分要尽可能保持数据分布的一致性，以避免由于分布的差异引入额外的偏差，常见的做法是采取分层抽样。同时，由于划分的随机性，单次的留出法结果往往不够稳定，一般要采用若干次随机划分，重复实验取平均值的做法。

- 交叉验证法

将数据集D划分为k个大小相同的互斥子集，满足$D=D_1∪D_2∪...∪D_k，D_i∩D_j=\emptyset（i≠j）$，同样地尽可能保持数据分布的一致性，即采用分层抽样的方法获得这些子集。交叉验证法的思想是：每次用k-1个子集的并集作为训练集，余下的那个子集作为测试集，这样就有K种训练集/测试集划分的情况，从而可进行k次训练和测试，最终返回k次测试结果的均值。交叉验证法也称“k折交叉验证”，k最常用的取值是10，下图给出了10折交叉验证的示意图。
<img src="picture\交叉验证.png" width = 80% height = 70% />

与留出法类似，将数据集D划分为K个子集的过程具有随机性，因此K折交叉验证通常也要重复p次，称为p次k折交叉验证，常见的是10次10折交叉验证，即进行了100次训练/测试。特殊地当划分的k个子集的每个子集中只有一个样本时，称为“留一法”，显然，留一法的评估结果比较准确，但对计算机的消耗也是巨大的。

-  自助法

我们希望评估的是用整个D训练出的模型。但在留出法和交叉验证法中，由于保留了一部分样本用于测试，因此实际评估的模型所使用的训练集比D小，这必然会引入一些因训练样本规模不同而导致的估计偏差。留一法受训练样本规模变化的影响较小，但计算复杂度又太高了。“自助法”正是解决了这样的问题。

自助法的基本思想是：给定包含m个样本的数据集D，每次随机从D 中挑选一个样本，将其拷贝放入D'，然后再将该样本放回初始数据集D 中，使得该样本在下次采样时仍有可能被采到。重复执行m 次，就可以得到了包含m个样本的数据集D'。可以得知在m次采样中，样本始终不被采到的概率取极限为：

$$ \lim\limits_{x\rightarrow\infty} (1-\frac{1}{m})^m \rightarrow \frac{1}{e} = 0.368$$

这样，通过自助采样，初始样本集D中大约有36.8%的样本没有出现在D'中，于是可以将D'作为训练集，D-D'作为测试集。自助法在数据集较小，难以有效划分训练集/测试集时很有用，但由于自助法产生的数据集（随机抽样）改变了初始数据集的分布，因此引入了估计偏差。在初始数据集足够时，留出法和交叉验证法更加常用。

## 2.4 调参

大多数学习算法都有些参数(parameter) 需要设定，参数配置不同，学得模型的性能往往有显著差别，这就是通常所说的"参数调节"或简称"调参" (parameter tuning)。

调参(parameter tuning): 在进行模型评估与选择时，还需要对算法参数进行设定。学习算法的很多参数在实数范围内取值，对每个参数都训练模型不太现实；通常采用的方法是“逐步搜索”法：即在参数的取值范围内，按照某个步长进行参数取值，并评估最好的参数设置。例如某参数取值在[0; 1] 内，可以
按照步长0.1 进行参数评估，共需评估11 个不同的参数设置。


最终模型：给定包含m 个样本的数据集D，在模型评估与选择过程中由于需要留出一部分数据进行评估测试，事实上我们只使用了一部分数据训练模型。因此，在模型选择完成后，学习算法和参数配置己选定，此时应该用数据集D重新训练模型. 这个模型在训练过程中使用了所有m 个样本，这才是我们最终
提交给用户的模型.

## 2.5性能度量
有一说一，这一段给我看傻了，都什么跟神马

性能度量就是衡量学习器泛化性能的标准，对比不同模型能力时，使用不同的性能度量往往会导致不同的评判结果；意味着模型好坏是相对的，模型好坏不仅取决于算法和数据还有任务需求。

评估性能就是要把学习器预测结果$f(x)$与真实标记$y$进行比较。

回归任务中，常用的性能度量就是均方误差：
$$ E(f;D) = \frac{1}{m} \sum _{i = 1}^m(f(x_i)-y_i)^2$$

知道概率密度的均方差描述为:
$$ E(f;D) = \int _{x~D} (f(x) - y)^2p(x)$$

下面介绍分类任务中的常见性能度量。
### 错误率和精度
- 错误率 ：分类错误的样本占总样本总数的比例，对数据集$D$，分类错误率定义：
  <img src="picture\错误率与精度.png" width = 70% height = 70% />
-  精度：分类正确的样本占样本总数的比例。对数据集$D$,精度为：
  <img src="picture\精度.png" width = 70% height = 70% />

对于二分类问题，将样例根据其真实类别（正例Positive 与反例Negative）与学习器预测类别的组合划分为四种情况：   
- 真正例 (True Positive, TP)：测试集中是Positive，模型预测结果是Positive 的数据。
- 假正例 (False Positive, FP) ：测试集中是Negative，模型预测结果是Positive 的数据。
- 假反例(False Negative, FN)：测试集中是Positive，模型预测结果是Negative 的数据。
- 真反例(True Negative, TN) ：测试集中是Negative，模型预测结果是Negative 的数据。

$TP + FP + FN + TN = 样本总数$,其中TP，TN 表示样本类别的分类正确；FP，FN 表示样本类别的分类错误。所以有：
$$ 错误率E = \frac{FN+FP}{TP+FN+FP+TN}\\
精度acc = \frac{TP+TN}{TP+FN+FP+TN}$$

### 查准率P与查全率R
上面错误率和精度主要侧重于猜对和猜错所占的比例，有时候我们关心正例而不要假例。比如，我们一般关心挑出的西瓜有多少比例是好瓜以及所有好瓜有多少比例挑出来了，这个对应的就是查准率和查全率，我们先把那个混淆矩阵整出来：
  <img src="picture\混淆矩阵.png" width = 70% height = 70% />
  $$ 查准率P = \frac{TP}{TP+FP}\\
查全率R = \frac{TP}{TP+FN}$$

上面式子中查准率就是指我现在预测了一堆好的结果，在这堆好的结果中真正好的结果所占的比例是多少。查全率是指真正好的结果中我预测出来的好的结果占好的结果的比例。

查全率和查准率是一对矛盾，一般感觉就是查的准确率高但是不一定全，全找出来了错的占比也就会高了。所以整一个$F_1$作为查准率和查全率的调和平均：
$$ \frac{1}{F_1} = \frac{1}{2}\cdot(\frac{1}{P}+\frac{1}{R})$$

现在考虑如何综合考虑查准率和查全率：
- P-R曲线
  先弄一个P-R曲线，根据学习器的预测结果对样本进行排序，最可能是正例的放在前面，按此顺序依次把样本最为正例进行预测，每次都计算当前查全率、查准率，得到P-R曲线。
   <img src="picture\P-R曲线.png" width = 60% height = 70% />

  如果一个P-R曲线完全保住另一个，那么这一个性能比较强，若有曲线交叉则不好判断

  平衡点为查准率=查全率时的取值。
  上面$F_1$为查全率和查准率综合考量，$F_1$更一般形式$F_\beta$:
  $$ F_\beta = \frac{(1+\beta ^2 \times P \times R)}{(\beta ^2 \times P)+R}$$

  其中$\beta$ > 0 度量了查全率对查准率的相对重要性
    - $\beta$> 1：查全率有更大影响；
    - $\beta$< 1：查准率有更大影响；
    - $\beta$= 1：退化为标准的F1.

  我们上面说的都是一个二分类混淆矩阵，现在假设我们有n个混淆矩阵，那么我们就有宏/微查准率和宏/微查全率，我直接弄图上去了。
  <img src="picture\查全率查准率.png" width = 100% height = 70% />
  
  两个的区别是对查全率、查准率平均还是对对应元素先平均。宏观就是先算出每个混淆矩阵的P值和R值，然后取得平均P值macro-P和平均R值macro-R，算出$F_β$或$F_1$，而微观则是计算出混淆矩阵的平均TP、FP、TN、FN，接着进行计算P、R，进而求出$F_β$或$F_1$。

### ROC与AUC
学习器预测样本一般是产生一个实数，将这个预测值和一个阈值进行比较，如果大于阈值判别为正，小于阈值判别为负。现在将所有预测值按照从大到小排序，那么这个阈值可以将预测值一分为二，前一部分判为正，后一部分判为负。所以我们就发现这个结果的好坏就取决于这个阈值设置了，不同的问题中显然阈值大小设置不一样，不同的需求设置截断点不一样(截断点就是阈值).重视查准率，靠前位置截断，重视查全率，靠后位置截断。排序本身的质量好坏体现了泛化性能的好坏，ROC曲线从这个角度来研究泛化性能。

和P-R曲线很类似，但是横纵坐标不太一样，横轴是假正例率(FPR)：也就是说反例中实际是正例所占的比例。纵轴是真正例率(TPR)：正例中实际上正例所占的比例：
$$ TPR = \frac{TP}{TP+FN}\\
FPR = \frac{FP}{TN+FP}
$$

实际上西瓜书对这一部分描述的很详细：
<img src="picture\ROC.png" width =80% height = 70% />

绘制ROC曲线图：
<img src="picture\ROC曲线绘制.png" width = 70% height = 70% />

- 若一个学习器的ROC 曲线被另一个学习器的的曲线完全‘包住’，则后者的性能优于前者；若两曲线交叉，则无法判断。
- y =x上的点表示一个采用随机猜测策略的分类器结果


如果两个ROC曲线有交点，采用AUC比较学习器性能优劣。

AUC表示ROC曲线下面的面积，AUC值越大，学习器性能越好。下面是如何计算AUC：
<img src="picture\计算AUC.png" width =50% height = 70% />

 ### 代价敏感错误率和代价曲线
 不同类型的错误所造成的后果不同，为权衡不同类型造成的不同损失，将不同类型的分类错误赋予非均等代价，如下面的二分类代价矩阵：
<img src="picture\二分类代价矩阵.png" width = 80% height = 70% />

非均等条件下的错误率为代价敏感错误率，考虑不同错误的代价：
$$ E(f;D;cost) = \frac{1}{m}(\sum _{x_i \in D^+}I(f(x_i) \neq y_i )\times cost_{01}+\sum _{x_i \in D^-}I(f(x_i) \neq y_i )\times cost_{10})$$

其中均等条件下为其特殊条件。

代价曲线能在非均等状态下反应学习器期望总体代价，下面是代价曲线极其绘制方法：
<img src="picture\代价曲线1.png" width = 80% height = 70% />
<img src="picture\代价曲线2.png" width = 70% height = 70% />


## 2.6 比较检验
这一段自己了解的不是很透彻，甚至是不了解，但是这一步部分的主要内容是利用适当的方法对对学习器的性能进行比较。也就是说通过前面的工作我们得到了一个学习算法的性能，下面我们来通过科学的方法对性能进行比较，感觉这个和概率论中的置信区间有点类似。也就是说若在测试机上观察到学习器A比学习器B好，则A的泛化性能是否在统计意义上比B好且这个结论的把握有多大。下面我们就来看看比较机器学习性能的方法，本篇中都是以“错误率”作为性能度量的标准。(感觉比较难，现在不掌握问题也不大，我们老师在pdf教案中都没有讲解这部分内容，这个在后续实验中应该会有所涉及)

下面的内容基本上剽窃GitHub上一位同学的分享。

### 假设检验

“假设”指的是对样本总体的分布或已知分布中某个参数值的一种猜想，例如：假设总体服从泊松分布，或假设正态总体的期望$u=u_0$。回到本篇中，我们可以通过测试获得测试错误率，但直观上测试错误率和泛化错误率相差不会太远，因此可以通过测试错误率来推测泛化错误率的分布，这就是一种假设检验。
<img src="picture\假设检验1.jpg" width = 100% height = 70% />
<img src="picture\假设检验2.png" width = 60% height = 70% />
<img src="picture\假设检验3.png" width = 60% height = 70% />

### 交叉验证t检验

<img src="picture\交叉验证1.png" width = 100% height = 70% />

###  McNemar检验

MaNemar主要用于二分类问题，与成对t检验一样也是用于比较两个学习器的性能大小。主要思想是：若两学习器的性能相同，则A预测正确B预测错误数应等于B预测错误A预测正确数，即$e_{01}=e_{10}，且|e_{01}-e_{10}|$服从$N（1，e_{01}+e_{10}）$分布。
<img src="picture\McNemar检验1.png" width = 50% height = 70% />
因此，如下所示的变量服从自由度为1的卡方分布，即服从标准正态分布$N（0,1）$的随机变量的平方和，下式只有一个变量，故自由度为1，检验的方法同上：做出假设-->求出满足显著度的临界点-->给出拒绝域-->验证假设。
<img src="picture\McNemar检验2.png" width = 50% height = 70% />

### Friedman检验与Nemenyi后续检验

上述的三种检验都只能在一组数据集上，F检验则可以在多组数据集进行多个学习器性能的比较，基本思想是在同一组数据集上，根据测试结果（例：测试错误率）对学习器的性能进行排序，赋予序值1,2,3...，相同则平分序值，如下图所示：
<img src="picture\Friedman检验与Nemenyi后续检验1.png" width = 75% height = 70% />

若学习器的性能相同，则它们的平均序值应该相同，且第i个算法的平均序值$r_i$服从正态分布$N（（k+1）/2，（k+1）(k-1)/12）$，则有：
<img src="picture\Friedman检验与Nemenyi后续检验2.png" width = 75% height = 70% />
<img src="picture\Friedman检验与Nemenyi后续检验3.png" width = % height = 70% />

服从自由度为k-1和(k-1)(N-1)的F分布。下面是$F$检验常用的临界值：
<img src="picture\Friedman检验与Nemenyi后续检验4.png" width = 80% height = 70% />


若“H0：所有算法的性能相同”这个假设被拒绝，则需要进行后续检验，来得到具体的算法之间的差异。常用的就是Nemenyi后续检验。Nemenyi检验计算出平均序值差别的临界值域，下表是常用的$q_\alpha$值，若两个算法的平均序值差超出了临界值域CD，则相应的置信度$1-α$拒绝“两个算法性能相同”的假设。
<img src="picture\Friedman检验与Nemenyi后续检验5.png" width = % height = 70% />
<img src="picture\Friedman检验与Nemenyi后续检验6.png" width = 90% height = 70% />


上面笔记总结基本copyGitHub上的，这部分内容当时我看着不是很懂，因此主要都是截图了。数学基础好的同学可以好好整一整，不够后面用到的话我还是会回来整整的。




## 2.7偏差与方差
方差：指的是同样大小的训练集的变动导致的学习性能的变化，刻画了数据扰动造成的影响，过拟合当训练集变动时，出现高方差。

偏差：学习算法的期望预测和真实结果的偏离程度，刻画算法本身的拟合能力，欠拟合高偏差。

噪声：表达了当前任务上任何算法所能达到的期望泛化误差的下界，即刻画了学习问题的本身难度。

- 偏差-方差分解
  <img src="picture\偏差方差分解.png" width = 80% height = 70% />



算法的期望泛化误差可以分解为偏差、方差和噪声之和。偏差方差分解说明泛化性能是由学习算法的能力、数据的充分性以及学习任务本身的难度所共同决定的。给定学习任务，时泛化性能变好需要使偏差和方差较小。

偏差和方差是有冲突的，训练不足时偏差主导泛化错误率，训练程度加深后学习器拟合能力变强，方差渐渐主导泛化错误率，程度充足后，学习器的拟合能力非常强，轻微扰动会主导学习器发生显著变化，发生过拟合。
 <img src="picture\偏差方差窘境.png" width = 60% height = 70% />


 到目前位置，我们的前戏就基本结束了，下面就是比较有意思的东西，涉及算法和理论知识还可以拿来练手，后面一变记笔记一边编程啦！