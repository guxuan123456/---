# 决策树算法
先来回顾一下，前面主要学习了线性模型。从单一属性开始弄出了最小二乘法，目标函数是最小化期望方差，求导之后可以得到相应参数的值，然后推广到多属性，采用向量形式求解。通过广义函数模型，将回归问题可以转化成分类问题，引入对数几率函数，通过梯度下降法求最优解，然后整个LDA将样本点进行投影，实现分类。最后讲了将多分类问题转化为二分类问题进行求解，最后弄了个类别不平衡问题的三种解决方法。在到最后我补充了梯度下降法一元二元的求解方法和原理。


现在来整一个相对比线性模型复杂一点的机器学习算法——决策树。决策树用于弄分类问题。属于监督学习算法。本身理解起来不算复杂，相当于通过属性对物体不断进行分类最终得到结果。典型的决策树算法主要有：ID3、C4.5、CART

## 基本流程
实际上决策树就是一个个if语句，满足条件走一条路，不满足条件走另外一条路，但最终都会走到叶结点，也就是分类结果。下面先整一个剽窃的例子：

      女儿：多大年纪了？
      母亲：26。
      女儿：长的帅不帅？
      母亲：挺帅的。
      女儿：收入高不？
      母亲：不算很高，中等情况。
      女儿：是公务员不？
      母亲：是，在税务局上班呢。
      女儿：那好，我去见见。

上面是一个选老公的决策问题。化成决策树就是这样的：
<img src="picture\相亲决策树.png" width = 80% height = 50% />

从上面的例子可以看出：

一棵决策树包含一个根结点、若干个内部结点、若干个叶结点。叶结点是最后的决策结果，其余结点都对应与某一个属性的测试（决策）。每个结点所包含的样本集合根据对属性的决策，往下划分到其的子结点中。根节点包含整个样本集，随着树的结点逐渐往下延伸，样本集也被分散到各个结点中，也就是说节点越向下，那个节点的样本就越小，最后直到到达叶结点，完成一个决策任务，相关概念总结如下。
- 每个非叶节点表示一个特征属性测试。
- 每个分支代表这个特征属性在某个值域上的输出。
- 每个叶子节点存放一个类别。
- 每个节点包含的样本集合通过属性测试被划分到子节点中，根节点包含样本全集。


### 决策树算法
- 算法形式
    Generate_decision_tree(D, A)。由给定的训练数据产生一棵判定树。
- 输入
    训练集 $D=(x_1,y_1), (x_2,y_2), ..., (x_m,y_m)$
    属性集 $A=\left\{a_1, a_2, ... , a_d\right\}$
    
- 输出
    返回一颗决策树
- 伪码
<img src="picture\决策树算法.png" width = 100% height = 50% />

决策树是一个不断调用递归函数的过程，其返回条件也就是到达叶结点的时候有3种情况：
- 当前结点包含的所有样本都是属于同一个类别，这时没有必要划分了。这个时候直接将该节点标记为叶节点，并设为相应的类别，因为这个时候只有一个类别啊，就是分完了。
- 当前属性集为空，或是所有样本在所有属性上取值相同，无法划分，这时将该节点标记为叶节点，并将其类别设为该节点所含样本最多的类别.
- 当前属性下所含的样本集合为空，即没有多的样本来划分了；这时也将该节点标记为叶节点，并将其类别设为父节点中所含样本最多的类别。

上面这个算法流程可能一开始没有接触感觉有些抽象，不过问题不大，我们需要结合具体的例子来理解的话就很好理解了，后面会有的，这里不知道也没关系感觉，我就感觉似懂非懂的。这里需要注意整个树的建立关键在于返回条件(递归程序返回条件先想好)和属性的划分。属性的划分就是个人感觉属性判断的前后顺序，就像前面先问是否是公务员和先问是否长得帅构建的树是不一样的。下面关键是属性划分了，在下面例子中更详细了解上面算法。

属性划分的目标是让各个划分出来的子节点尽可能地“纯”，即属于同一类别，感觉这样树构造的比较小。因此下面便是介绍量化纯度的具体方法，决策树根据属性划分方式不同最常用的算法有三种：ID3，C4.5和CART。


## 属性划分依据，划分选择

### 信息增益(information gain)-ID3算法
ID3是根据信息增益弄得一个算法，接下来我们一步步介绍出信息增益。

- 信息熵(information entropy)
  信息熵是度量样本结合纯度的常用指标，这个我们在信息论中学到过。假定当前样本集合$D$中第$k$类样本所占比例为$p_k$，则样本集合$D$的信息熵定义为：
    $$Ent(D) = -\sum_{k=1}^{\left|\gamma\right|}p_klog_2p_k$$

  信息熵越小不确定性越小,也就是说上式越小则纯度越高，我们可以这样理解。学过信息论直到上面那个信息熵在所有样本等概率分布时最大，此时各种类型都有，没有倾向性，此时我们可以说他纯度低，因为他啥都有。这尼玛就是个海王、渣男。所以总是希望信息熵较小，这样就很纯了。

- 信息增益
  
  现在我们开始分了，假定离散属性$a$有$V$个可能的取值${a^1,a^2,...,a^V}$,使用a对样本集$D$进行划分，会产生$V$个分支节点，其中第$v$个分支节点包含了$D$所在属性a上取值为$a^V$的样本，记为$D^v$。此时可以算出$D^v$的信息熵。考虑到不同分支节点包含的样本数不同，所占的权重也不同，所以可以将分支结点赋予权重$\frac{|D^v|}{|D|}$。这样可以算出属性a对数据集D进行划分后所获得信息增益：
  $$ Gain(D,a) = Ent(D) - \sum_{v=1}^V\frac{|D^v|}{|D|}Ent(D^v)$$

  这就比较清楚了，我们要找的肯定就是信息增益最大的，这样我们就知道如何选择划分属性了，也就是上面伪码中的选择属性：$a_*= \underset{a \in A}{\operatorname{arg \, max}} Gain(D,a)$,上面这个也是我们的最优化目标，这样就能构建决策树了。

下面弄一个西瓜树上的例子，方便理解处理过程和前面的那个算法，其它的算法跟着类似，就是属性划分时不太一样：
1. 数据集
<img src="picture\决策树举例1.png" width = 80% height = 50% />

2. 算法流程
<img src="picture\决策树举例2.png" width = 80% height = 50% />
<img src="picture\决策树举例3.png" width = 80% height = 50% />
<img src="picture\决策树举例4.png" width = 80% height = 50% />

3. 生成决策树
<img src="picture\决策树举例5.png" width = 80% height = 50% />



### 增益率(Gain Ratio)-C4.5算法
ID3算法存在一个问题，就是偏向于取值数目较多的属性，这样有最大的信息增益，例如：如果存在一个唯一标识，这样样本集D将会被划分为$|D|$个分支，每个分支只有一个样本，这样划分后的信息熵为零，十分纯净，但是对分类毫无用处，不具有泛化能力。

基于上面的思考我们整出了增益率来弄C4.5算法。利用增益率来进行属性划分。
<img src="picture\增益率.png" width = 80% height = 50% />

上面$IV(a)$称为属性a的固有值，一般属性a的可能性取值数目越多(V越大)，$IV(a)$的值通常越大。这样的话就可以将分的多的信息增益降低。

但是增益率准则对可取值数目较少的属性有偏好，这样的分母较小。所以C4.5不是直接采用将增益率取最大值的方法，而是先从候选划分中找到信息增益高于平均水平的属性，在从当中选择增益率最高的。这样可以综合增益率和信息增益的优点。
### 基尼指数(Gini index)-CART算法
ART决策树使用“基尼指数”（Gini index）来选择划分属性，这个算法应用较多分类和回归任务都可用，基尼指数反映的是从样本集D中随机抽取两个样本，其类别标记不一致的概率，因此$Gini(D)$越小越好，这样其纯度较高，基尼指数定义：
<img src="picture\基尼指数1.png" width = 80% height = 50% />

划分之后，属性a的基尼指数定义为：
<img src="picture\基尼指数2.png" width = 80% height = 50% />



最优化目标函数为$a_*= \underset{a \in A}{\operatorname{arg \, min}} Gini_index(D,a)$

## 剪枝处理
不管怎么样的训练集，决策树总是能很好地将各个类别分离开来，这时就会遇到之前提到过的问题：过拟合（overfitting），即太依赖于训练样本，将不是一般的特征也整出来了。剪枝（pruning）则是决策树算法对付过拟合的主要手段，剪枝的策略有两种如下：
  - 预剪枝（prepruning）：在构造的过程中先评估，再考虑是否分支。
  - 后剪枝（post-pruning）：在构造好一颗完整的决策树后，自底向上，评估分支的必要性。

现在就是如何评估泛化能力啦，可以使用测试集作为学习器泛化性能的近似。考虑采用留出法将数据集分为训练集和测试集。预剪枝表示在构造数的过程中，对一个节点考虑是否分支时，首先计算决策树不分支时在测试集上的性能，再计算分支之后的性能，若分支对性能没有提升，则选择不分支（即剪枝）。后剪枝则表示在构造好一颗完整的决策树后，从最下面的节点开始，考虑该节点分支对模型的性能是否有提升，若无则剪枝，即将该节点标记为叶子节点，类别标记为其包含样本最多的类别。

下面我们举个例子来说明预剪枝和后剪枝，两个剪纸我们都以信息增益作为准则。数据集划分如下:
<img src="picture\剪枝操作.png" width = 80% height = 50% />

尚未剪枝时的未剪枝决策树：
<img src="picture\未剪枝决策树.png" width = 80% height = 50% />

### 预剪枝
预剪枝的流程就是先算出划分前和划分后的分类正确比率，看比率是否上升，上升就划分节点，不然就不划分节点。具体过程如下图所示：
<img src="picture\预剪枝流程.png" width = 100% height = 50% />

这样继续以同样方式划分节点$D_1,D_2$，最终所得的结果如下所示：
<img src="picture\预剪枝决策树.png" width = 100% height = 50% />

从图中对比可以看出预剪枝的优缺点：
预剪枝优点：可以使得很多决策树的很多分支没有展开，这样可以降低过拟合风险，显著减少训练开销和测试时间开销。
预剪枝缺点：但是另一方面有些分支当前划分虽然可能会降低泛化性能，但其后续划分可能显著提高泛化性能，所以啊我们看到预剪枝基于贪心本质禁止分支展开会带来欠拟合的风险。

### 后剪枝
后剪枝是先训练出来一颗决策树，如上面未剪枝的图，然后再自底向上不断进行考察。上面图中剪枝前精度为42.9%
上面先对纹理进行剪枝，看看剪枝后是否提升，发现变为57.1%>42.9%。说明又提升了，结果剪枝。

其流程及结果如下图所示：
<img src="picture\后剪枝决策树.png" width = 100% height = 50% />

可以看出后剪枝的优点：后剪枝决策树的欠拟合风险很小，泛化性能往往优于预剪枝决策树。
后剪枝缺点：后剪枝过程是在生成完全决策树之后进行的，并且要自底向上对树中所有非叶节点逐一考察，因此训练时间开销比未剪枝和预剪枝决策树要大得多。
## 连续与缺失值
### 连续值处理

上面我们讨论的属性划分都是基于离散属性的假设，但是如果数据集包含密度、含糖量等连续属性，怎末进行决策树的构建呢。

考虑将连续属性离散化，比如采用二分法把连续属性化作两个区间，C4.5决策树采用了这种方法。

下面是定义了：给定样本集$D$和连续属性$a$,假定a在D上出现了$n$个不同的取值，将这些值按照从小到大进行排序，记为${a^1,a^2,...,a^n}$，基于划分点t可将$D$分为子集$D_t^-$和$D_t^+$,其中$D_t^-$包含那些在属性a上取值不大于$t$的样本，$D_t^+$则包含哪些属性a上取值大于t的样本，对相邻属性取值$a^i与a^{i+1}$来说，t在区间$[a^i,a^{i+1})$中取任意值所产生的划分结果相同.因此，对连续属性a，可考察包含n − 1 个元素的候选划分点集合:
$$ T_a = {\frac{a_i+a_{i+1}}{2} |1 \leq i \leq n-1 }$$

把区间$[a_i,a_{i+1})$的中位点$\frac{a_i+a_{i+1}}{2}$作为划分点，即可以像离散属性一样来考察这些划分点，从候选集合$T_a$中选择最佳的划分点$t_m$进行划分。

下面看看连续属性的信息增益Gain(D,a):
<img src="picture\连续值处理.png" width = 80% height = 50% />


下面是一个连续值处理的具体例子和步骤：
<img src="picture\连续值处理1.png" width = 90% height = 50% />


### 缺失值处理
其实在现实任务中可能会出现数据集含有缺失值的情况，但是这些样本和属性不能删除，这就是缺失值处理，缺失值处理需要解决两个问题：
1. 训练阶段：如何在属性值缺失的情况下进行划分属性选择？
2. 验证（剪枝）阶段及测试阶段：给定划分属性，若样本在该属性上的值缺失，如何对样本进行划分？

上面问题可以这样解决，(有点懒，直接贴了个图)：
<img src="picture\缺失值处理1.png" width = 100% height = 50% />

下面我们来整个西瓜书上的例子，以便更好理解缺失值处理：
数据集：
<img src="picture\缺失值处理2.png" width = 80% height = 50% />
训练过程：
<img src="picture\缺失值处理3.png" width = 80% height = 50% />
<img src="picture\缺失值处理4.png" width = 80% height = 50% />
训练结果：
<img src="picture\缺失值处理5.png" width = 80% height = 50% />


## 多变量决策树
若我们将每个属性看成是坐标空间中的一个坐标轴，则d个属性描述的样本对应了d维空间中的一个数据点，对样本进行分类则意味着在这个坐标空间中寻找不同样本类别之间的分类边界。决策树所形成的分类边界有一个明显的特点：轴平行(axis-parallel)，即它的分类边界由若干个与坐标轴平行的分段组成。

当边界分类比较复杂是，比如是条斜线或者是曲线，需要使用多段划分才能取得较好的近似。下面是一个例子：
<img src="picture\多变量决策树1.png" width = 100% height = 50% />

多变量决策树实际上实现斜的划分边界，把不断分段转化为一条曲线将大大简化决策树，这样不是针对某个属性进行划分而是针对属性的线性组合进行划分，相当于弄了一个线性分类器，在二维坐标系上表现为直线将两个东西分开，相关概念和决策流程如下：
<img src="picture\多变量决策树2.png" width = 90% height = 50% />

好的基本知识就是这些，这章内容编程还是蛮多，下面搞一搞，一下章就是冲刺神经网络了，兄弟们雄起。