# 支持向量机
前面介绍了神经网络，了解到神经网络的相关概念，只要神经网络够复杂理论上可以构建任何的分类器。首先整了一下感知机模型，基于梯度下降法描述感知机模型的权值调整规则。简单的感知机无法处理非线性划分，便引入了含隐层的前馈型神经网络，BP神经网络是最为成功的一种学习方法，采用误差逆传播逐层调节连接权。然后是几个典型的神经网络和深度学习的概念。接下来学习一种监督学习算法-支持向量机(SVM),这个名字不知道听说了多少次了已经，终于要学习了！


## 间隔与支持向量

支持向量机是一种经典的二分类模型，基本模型定义为特征空间中最大间隔的线性分类器，其学习的优化目标便是间隔最大化，因此支持向量机本身可以转化为一个凸二次规划求解的问题。

### 支持向量
对于二分类学习，假设现在的数据是线性可分的，这时分类学习最基本的想法就是找到一个合适的超平面，该超平面能够将不同类别的样本分开，类似二维平面使用ax+by+c=0来表示，超平面实际上表示的就是高维的平面，如下图所示：
<img src="picture\支持向量机1.png" width = 60% height = 50% />

下面是一些相关的概念：
样本空间中1划分超平面的线性方程如下：
$$ w^Tx+b=0$$

其中$w= (w_1;w_2;...;w_d)$为法向量，决定超平面方向；b为位移项，觉得超平面和原点的距离。

样本空间任一点$x$到超平面$(w,b)$
距离：
$$ r=\frac{|w^Tx+b|}{||w||}$$

假设这个平面能将样本分类正确：
$$
\left \{ 
\begin{array}{l}
w^Tx_i+b \geq 1,y_i =1 \\
w^Tx_i+b \leq -1,y_i =-1
\end{array}
\right.
$$

距离超平面最近的几个训练样本点使上式成立，被称为支持向量，两个支持向量到超平面距离之和：
$$ r = \frac{1}{||w||}$$

上式称为间隔。其中支持向量和间隔表示如下：
<img src="picture\支持向量与间隔.png" width = 100% height = 50% />

现在想要找到具有间隔的划分平面，就是满足：
$$ max \ \frac{1}{||w||} $$
约束条件：$$ s.t. y_i(w^Tx_i+b) \geq 1,i = 1,2,...,m$$
为了方便，上式等价为：
$$ min \ \frac{1}{2}||w||^2 $$
约束条件：$$ s.t. y_i(w^Tx_i+b) \geq 1,i = 1,2,...,m$$

 这就是支持向量机的基本型。

## 对偶问题
上面挣了基本型，希望通过求解基本型来得到大间隔划分超平面对应的模型：$f(x) = w^Tx+b$,这是一个凸二次规划问题，下面是如何求解。

求解我们通过对偶问题进行求解，这样更容易求解而且出现了向量内积的形式，从而能更加自然的引出核函数。对偶问题可以理解成优化等价的问题，更一般地，是将一个原始目标函数的最小化转化为它的对偶函数最大化的问题。对于当前的优化问题，首先我们写出它的朗格朗日函数，对基本型添加拉格朗日乘子$\alpha _i \geq 0$(其中非支持向量的$\alpha_i=0$)

$$ L(w,b,\alpha) = \frac{1}{2}||w||^2 + \sum_{i=1}^m \alpha_i (1-y_i(w^Tx_i+b))$$

其中$\alpha = (\alpha_i;\alpha_2;...;\alpha_m)$.现在要求上面的最大值就可以得到对偶问题，但是上面那个式子有三个约束变量，现在考虑消去两个变量变为单变量。将$L(w,b,\alpha)$对$w,b$求偏导得到：

$$ w = \sum_{i=1}^m \alpha _i y_i x_i
   0 =  \sum_{i=1}^m \alpha _i y_i
$$

将上面式子带入拉格朗日函数可以得到：
<img src="picture\对偶问题.png" width = 80% height = 50% />

只需满足约束条件：$\sum_{i=1}^m \alpha_iy_i = 0;\alpha_i \geq 0,i = 1,2,...,m$。利用上面的式子求出$\alpha$之后可以得到$w，b$：
$$ f(x) = w^Tx+b = \sum_{i=1}^m \alpha_i y_i x_i^Tx+b$$

上面式子如何求解，一般采用SMO算法，后面我们介绍SMO算法。

前面的式子，也就是最开始的支持向量机的基本型满足KKT约束条件：
$$
\left \{ 
\begin{array}{l}
\alpha_i \geq 0 \\
y_i f(x_i)-1 \geq 0\\
\alpha_i(y_i f(x_i)-1 ) =0
\end{array}
\right.
$$

上式说明了要么$\alpha_=0$,要么$y_if(x_i) = 1$，如果$\alpha_=0$，$f(x)$表达式中该项求和不会出现，对$f（x）$没有影响，也就是不是在支持向量上，如果$\alpha_i >0 $，所对应的样本点位于支持向量上，这说明上面模型训练完毕后大部分的训练样本不需要保留，最终模型核支持向量有关。

#### SMO算法
基本思路是先固定$\alpha_i$之外的所有参数，然后求取$\alpha_i$上的极值。SMO执行下面两个步骤直至收敛：
- 选取一对需更新的变量$\alpha_i和\alpha_j$
- 固定$\alpha_i和\alpha_j$之外的参数，求解前面关于$\alpha_i$的约束式获得更新后的$\alpha_i和\alpha_j$

这里注意到如果$\alpha_i和\alpha_j$有一个不满足KKT条件，目标函数在迭代后会减小，一般KKT条件违背程度越大，减小程度越大，为了使目标函数变化较大，SMO算法使用了一个启发式：使选取的两个变量所对应的样本之间的间隔最大，这样两个变量有很大的差别与两个相似的变量进行更新比较，对他们进行更新带来目标函数值更大的变化，下面说说SMO算法的高效原因：

现在我们只选取两个参数$\alpha_i和\alpha_j$，那么式子的约束条件变为
$$ \alpha_iy_i +\alpha_j y_j = c ,\alpha_i \geq 0 ,\alpha_j \geq 0$$

其中$c= - \sum_{k \neq i,j} \alpha_k y_k$是使$\sum_{i=1}^m\alpha_iy_i=0$成立的常数。

这样就可以消去一个变量$\alpha_j$变成一个单变量二次规划问题，约束为$\alpha_i \geq 0$.

现在来看如何算偏移项b,对任意支持向量都有$y_sf(x_s) = 1$,即：
$$ y_s(\sum_{i \in S}\alpha_iy_ix_i^Tx_s+b) = 1$$

其中$S$为所有支持向量的下标集，所以上面式中任何支持向量上的点都可以求出b，也可以用一个更鲁棒性的方法，求取平均值：
$$ b = \frac{1}{|S|} \sum_{s \in S}(y_s-\sum_{s \in S}\alpha_iy_ix_i^Tx_s)$$


## 核函数
上面弄了训练样本是线性可分的，现在假设并不是线性可分的，可以映射到高维空间变成线性可分的，这被证明是一定成立的。

现在令$\phi (x)$表示$x$映射后的特征向量，于是特征空间中划分超平面所对应的模型表示为：
$$ f(x) = w^T \phi (x)+b$$

这样的话相当于将前面式子中$x$换成$\phi (x)$就可以得到现在的表达式。然后就有下面这些东西：

- 原支持向量基本型

$$ min \ \frac{1}{2}||w||^2 $$
约束条件：$$ s.t. y_i(w^T \phi (x_i)+b) \geq 1,i = 1,2,...,m$$

- 拉格朗日函数变换后的对偶问题
  <img src="picture\核函数对偶问题1.png" width = 60% height = 50% />
- 原分类函数变为
  <img src="picture\核函数分类方程.png" width = 60% height = 50% />

上面的求解过程中涉及$\phi(x_i)^T\phi(x_j)$这是样本映射到高维空间之后的内积，由于维数可能很高，所以计算十分复杂，现在我们用一个核函数代替这个内积：

$$ \kappa(x_i,x_j) = <\phi(x_i),\phi(x_j)> = \phi(x_i)^T\phi(x_j)$$

于是就有：
1. 对偶问题：
  <img src="picture\核函数对偶问题.png" width = 60% height = 50% />
2. 分类函数：
  <img src="picture\核函数分类方程.png" width = 60% height = 50% />


这样模型最优解可通过训练样本核函数展开，称为“支持向量展开式”。知道$\phi(\cdot)$可以知道核函数，但大多情况下不知道，现在来研究核函数的性值。

下面是核函数定义：
<img src="picture\核函数定义.png" width = 80% height = 50% />

上述定理表明，只要一个对称函数所对应的核矩阵半正定，就能作为核函数使
用。事实上，对于一个半正定核矩阵，总能找到一个与之对应的映射ϕ。
<img src="picture\常用核函数.png" width = 80% height = 50% />

另外还可以通过函数组合获得，例如：
<img src="picture\核函数1.png" width = 80% height = 50% />
<img src="picture\核函数2.png" width = 80% height = 50% />

## 软间隔与正则化
下面一部分话是copy的，前面的讨论中，我们主要解决了两个问题：当数据线性可分时，直接使用最大间隔的超平面划分；当数据线性不可分时，则通过核函数将数据映射到高维特征空间，使之线性可分。然而在现实问题中，对于某些情形还是很难处理，例如数据中有噪声的情形，噪声数据（outlier）本身就偏离了正常位置，但是在前面的SVM模型中，我们要求所有的样本数据都必须满足约束，如果不要这些噪声数据还好，当加入这些outlier后导致划分超平面被挤歪了，如下图所示，对支持向量机的泛化性能造成很大的影响。
<img src="picture\软间隔1.png" width = 60% height = 50% />

为了解决这一问题，我们需要允许某一些数据点不满足约束，即可以在一定程度上偏移超平面，同时使得不满足约束的数据点尽可能少，这便引出了“软间隔”支持向量机的概念

- 允许某些数据点不满足约束$y(w'x+b)≥1$；
- 同时又使得不满足约束的样本尽可能少。

前面所有样本都必须划分正确，这时硬间隔，现在允许某些样本不满足条件，这时软间隔。

最大化间隔同时，不满足约束的样本尽可能少，优化目标为：
<img src="picture\损失函数.png" width = 60% height = 50% />

上式当$C$为无穷大时，迫使所有样本均满足约束，这就和前面硬间隔相同，当$C$取有限值时，允许一些样本不满足约束。

但是$l_{0/1}$非凸、非连续。优化目标不好求解，考虑用其他函数代替$l_{0/1}$，称为“替代损失”，常见的几种替代损失函数。
<img src="picture\替代损失函数.png" width = 80% height = 50% />

如果我们采用hinge损失，优化目标变为：
$$ min \, \frac{1}{2}||w||^2 +C\sum_{i=1}^max(0,1-y_i(w^Tx_i+b))$$

如果引入松弛变量$\varepsilon_i \geq 0$，感觉是代替上式后面的部分，所得的"软间隔支持向量机"如下所示：
<img src="picture\软间隔支持向量机.png" width = 60% height = 50% />

其中C为一个参数，控制着目标函数与新引入正则项之间的权重，这样显然每个样本数据都有一个对应的松弛变量，用以表示该样本不满足约束的程度，将新的目标函数转化为拉格朗日函数得到：
<img src="picture\软间隔拉格朗日函数.png" width = 80% height = 50% />

其中$\alpha_i \geq 0,\mu_i \geq 0$是拉格朗日乘子。同理先让$L求关于w，b$以及松弛变量的极小，再使用SMO求出$\alpha$，有：
<img src="picture\软间隔偏导.png" width = 60% height = 50% />

带入前面的拉格朗日函数得到对偶问题：
<img src="picture\软间隔对偶问题.png" width = 70% height = 50% />

将“软间隔”下产生的对偶问题与原对偶问题对比可以发现：新的对偶问题只是约束条件中的α多出了一个上限C，其它的完全相同，因此在引入核函数处理线性不可分问题时，便能使用与“硬间隔”支持向量机完全相同的方法。也就是说前面的计算方法只需要多设置一个$C$,仍然可以采用前面提到的SMO算法来求解。相应的软间隔KKT条件为：
$$
\left \{ 
\begin{array}{l}
\alpha_i \geq 0 ，\mu _i \geq 0 \\
y_i f(x_i)-1 + \varepsilon_i \geq 0\\
\alpha_i(y_i f(x_i)-1 + \varepsilon_i ) =0 \\
\varepsilon_i \geq 0,\mu_i \varepsilon_i = 0
\end{array}
\right.
$$

最终模型仅和支持向量有关，原来的支持向量机就是按照划分的那个支持向量作为边界，在哪一侧便属于哪一类，$\alpha_i \geq 0 $,样本点分布在最大间隔边界上，就是那一条直线，变成软间隔之后，有数据可以位于最大间隔内部，这样相当于容错性变好了。

根据上面的KKT条件，若$\alpha_i = 0$，该样本不会对$f(x)$产生影响。若$\alpha_i >0$,此时有$y_if(x_i) = 1- \varepsilon_i $,该样本是支持向量，若$\alpha_i <C $,该样本恰在最大间隔边界上；若$\alpha_i= C $,样本落在最大间隔内部。

前面使用其它替代损失函数时都有一个共性，优化目标的第一项用来描述划分超平面的“间隔”大小，另一项用来表示训练集上的误差，写为一般的形式：
$$ \underset{f}{min} \  \Omega(f) + C \sum_{i=1}^m l(f(x_i),y_i)$$

式中$\Omega(f)$称为结构风险，用于描述模型的某些性质；第二项称为经验风险，用于描述模型和训练数据得契合程度，C用于对二者进行折中。

上式也叫正则化问题，$\Omega(f)$称为正则化项，C称为正则化常数。

## 支持向量回归
现在就是得到那个支持向量的那个曲线，当$f(x)$和$y$完全相同时，损失为0，现在假设允许$f(x)和y$有一定偏差，那就弄成了支持向量回归。
<img src="picture\支持向量回归1.png" width = 80% height = 50% />
<img src="picture\支持向量回归2.png" width = 80% height = 50% />
<img src="picture\支持向量回归3.png" width = 80% height = 50% />

上面拉格朗日函数对各部分求偏导：
<img src="picture\支持向量回归4.png" width = 80% height = 50% />

满足的KKT条件：
<img src="picture\支持向量回归5.png" width = 80% height = 50% />

上面的东西如果考虑维度变换，整一个核函数出来，就是换成$\kappa (x,x_i)$.

## 核方法
这是一个很抽象的东西，相当于从前面的方程中总结出的一般规律，能通过这个规律求解一类核函数问题，它可以将线性判别分析通过核化进行非线性拓展。
<img src="picture\核函数.png" width = 80% height = 50% />


上面基本介绍完了支持向量机，看完之后总体感觉很迷糊，看完之后还是有许多地方不是很明白(数学功底不够)。但是支持向量机主要就是二分类问题中找到一个比较好的直线，使位于两个间隔距离较大这样不容易弄叉屁，不是弄直线的化就是改变维度变成直线。具体实现过程不太能记住，估计就是要自己动手编程之后对问题得到进一步地掌握了。虽然没懂但是接着肝吧，这个理解起来确实不好搞。